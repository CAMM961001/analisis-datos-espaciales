---
title: "Grandes datos"
output: html_document
date: "2023-02-21"
---

Carga de librerías

```{r, echo=FALSE}
library(foreign)
library(tidyverse)
library(lubridate)
library(readr)
library(sp)
library(rgdal)
library(geosphere)
library(leaflet)
library(readxl)
library(RCurl)
```

Es momento de empezar a pensar en el cómo podemos manejar grandes cantidades de información. Como se vio en las lecciones anteriores, los datos espaciales están hechos de tal manera que rápidamente pueden saturar la memoria de los equipos, y no siempre tendremos a disposición recursos de cómputo ilimitados. Por eso, definir estrategias para manejar grandes cantidades de información es importante.

Para esta lección se van a utilizar datos de crímenes registrados en la Ciudad de México, mismos que se puede extraer del portal de [datos abiertos](https://datos.cdmx.gob.mx/dataset/carpetas-de-investigacion-pgj-cdmx) de la ciudad.

```{r}
delitos <- paste0(getwd(), '/datos/delitos/Delitos.csv')
A <- read_csv(delitos)

# Filtrar registros válidos
B <- filter(A, latitud != 0 & longitud != 0)

# Liberar memoria
rm(A)

dim(B)
```

Observamos que se trata de un conjunto significativamente grande.

# Tiempo de procesamiento

Supongamos que se quiere ejecutar un algoritmo en el que se nos pide determinar la distancia del primer punto a los subsecuentes $n$. Una forma de conseguirlo es utilizando lógica de programación básica, como pudiera ser un ciclo for:

```{r}
for (i in 1:10){
  out <- distHaversine(
    c(B$longitud[i], B$latitud[i]),
    c(B$longitud[1],B$latitud[1]),
    r=6378137)
  print(c(i, out))
}
```

Pero ¿qué pasa cuando el número de puntos aumenta? Para ilustrarlo podemos pensar en el siguiente código:

```{r}
# Vector para tiempo de procesamiento
tiempo <- c()

# 100 puntos
a <- now()

for (i in 1:100){
  distHaversine(
    c(B$longitud[i], B$latitud[i]),
    c(B$longitud[1],B$latitud[1]),
    r=6378137)
}
# Registrar tiempo
a <- now() - a
tiempo <- rbind(tiempo, a %>% as.numeric())

# 1,000 puntos
a <- now()
for (i in 1:1000){
  distHaversine(
    c(B$longitud[i], B$latitud[i]),
    c(B$longitud[1], B$latitud[1]),
    r=6378137)
}
# Registrar tiempo
a <- now() - a
tiempo <- rbind(tiempo, a %>% as.numeric())

# 10,000 puntos
a <- now()
for (i in 1:10000){
  distHaversine(
    c(B$longitud[i],B$latitud[i]),
    c(B$longitud[1],B$latitud[1]),
    r=6378137)
}
# Registrar tiempo
a <- now() - a
tiempo <- rbind(tiempo, a %>% as.numeric())

# 100,000 puntos
a <- now()
for (i in 1:100000){
  distHaversine(
    c(B$longitud[i],B$latitud[i]),
    c(B$longitud[1],B$latitud[1]),
    r=6378137)
}
# Registrar tiempo
a <- now() - a
tiempo <- rbind(tiempo,a %>% as.numeric())
```

Observamos gráficamente el comportamiento del tiempo de procesamiento:

```{r}
plot(tiempo)
lines(tiempo)
```

Lo primero que destaca es que el tiempo de proceso no aumenta linealmente con el número de puntos, más bien tiene un comportamiento exponencial. Otra forma de verlo es comparando la proporción de tiempo que aumenta con cada paso:

```{r}
c(tiempo[2]/tiempo[1], tiempo[3]/tiempo[2], tiempo[4]/tiempo[3])
```

Vemos que las proporciones no aumentan linealmente, lo cual es un indicador de que debemos tener cuidado con los recursos computacionales ya que fácilmente puede aumentar el costo computacional.

# Optimización de cálculos

Si bien como se mencionó, una forma de atender este problema sería teniendo recursos computacionales infinitos, esto no es una solución viable por diversos motivos. Lo más adecuado (Y lo más elegante) es proponer lógica de filtrado de información para optimizar los cálculo.

Para ilustrarlo, supón que te solicitan determinar **cuantos** delitos están a **menos de 100 metros** de los primeros 5 delitos registrados en la base `B`. Podrías resolverlo utilizando lógica de programación básica de la siguiente manera:

```{r}
# Objeto para almacenar datos
total <- c()

# Estampa de tiempo
a <- now()

# Procesamiento
# Primeros 5 delitos
for(i in 1:5){
  # Contador de delitos
  vec <- 0
  
  # Condición de distancia
  for(j in 1:nrow(B)){
    if(
      (distHaversine(
        c(B$longitud[j], B$latitud[j]),
        c(B$longitud[i], B$latitud[i]),
        r=6378137) < 100)
      )
      {vec <- vec + 1}
  }
  # Acumulamos los delitos cercanos por cada uno
  total <- rbind(total,vec)
  print(i)
}

# Medimos el tiempo de procesamiento
a_ciclo <- now() - a
a_ciclo
```

Vemos que efectivamente hace el trabajo, pero es sumamente ineficiente. Podemos mejorar esta situación aprovechando algunas características de lo que hemos visto hasta heste punto.

## Filtrado de datos - Criterio del cuadrado

Recordemos que la proyección UTM Mercator es una proyección cartográfica **cartesiana**, es decir, sus coordenadas se pueden representar y manipular con herramientas geométricas convencionales, que para el caso de la distancia, se puede utilizar una **distancia euclideana**. Teniendo esto en cuenta, se puede definir una circunferencia de radio $r$ inscrita en un cuadrado de longitud por lado de $2r$, y centrada en el punto de referencia, por lo que se podrían filtrar aquellos puntos de la base que se encuentren en la región:

$$x\in\{x_0-r, x_0+r\}$, $y\in\{y_0-r, y_0+r\}$$

Una vez filtrados estos puntos, entonces se puede ejecutar la misma lógica definida anteriormente. Antes de continuar, primero definimos una función para determinar la **distancia euclidiana** entre dos puntos:

```{r}
euclidean <- function(a, b) {sqrt(sum((a - b)^2))}
```

Para realizarlo, como siempre, empezamos convirtiendo los datos de la proyección en la que se encuentren a la proyección deseada (UTM Mercator en este caso):

```{r}
# Objeto con coordenadas
coords <- data.frame(lon=B$longitud, lat=B$latitud)
coordinates(coords) <- c("lon", "lat")

# Proyección inicial
sp_utm <- SpatialPoints(coords, proj4string=CRS("+proj=longlat +datum=WGS84"))

# Proyección final
sp_latlon <- CRS("+proj=utm +zone=14 +datum=WGS84 +units=m +no_defs") 

# Transformación de datos
sp_geo <- spTransform(sp_utm, sp_latlon)
sp_geo <- as.data.frame(sp_geo)

# Tenemos un equivalente como "lat,lon" en coordendas en el plano cartesiano
colnames(sp_geo) <- c("lon_UTM","lat_UTM")
head(sp_geo)
```

Una vez en este punto, hacemos el procesamiento de los datos como se describió, y comparamos el tiempo:

```{r}
# Objeto para almacenar datos
total2 <- c()

# Estampa de tiempo
b <- now()
# Procesamiento
# Primeros 5 delitos
for(i in 1:5){
  # Contador de delitos
  aux <- 0
  
  # Filtrar delitos en el cuadrado (Valor absoluto)
  vec1 <- which( abs(sp_geo$lat_UTM[i] - sp_geo$lat_UTM) < 100)
  vec2 <- which( abs(sp_geo$lon_UTM[i] - sp_geo$lon_UTM) < 100)
  vec <- intersect(vec1,vec2)
  
  # Filtrar delitos en el cuadrado pero fuera del círculo
  if(length(vec) > 0){
    for(j in 1:length(vec)){
      if( 
        (euclidean(
          c(sp_geo$lon_UTM[i], sp_geo$lat_UTM[i]),
          c(sp_geo$lon_UTM[vec[j]], sp_geo$lat_UTM[vec[j]])) < 100) )
        {aux <- aux + 1}
    }
  }
  # Acumular delito en contador
  total2 <- rbind(total2, aux)
  print(i)
}

# Tiempo de procesamiento
b_ciclo <- now() - b
b_ciclo

```

Observamos que efectivamente hace el trabajo, y además lo hace en un tiempo significativamente menor al del proceso anterior.

Sin embargo, si bien esto es mucho más eficiente, aún no es del todo correcto. ¿Qué observas si comparas los resultados del primero proceso y el segundo? Toma en cuenta lo siguiente:

-   El primer proceso, si bien es ineficiente, sus resultados son confiables pues se determinaron las distancias con `Haversine`, es decir, directamente en coordenadas geográficas.
-   El segundo, si bien es eficiente, sus resultados se determinaron con distancias en proyección UTE Mercator, es decir, hay errores en los cálculos por la proyección en sí misma.

Observamos los resultados:

```{r}
cbind(total, total2)
```

Y efectivamente, existen diferencias en los cálculos.

## Filtrando en coordenadas geográficas

Se puede corregir la situación anterior pensando en lo siguiente:

-   En coordenadas geográficas el cálculo de distancia usando `Haversine` es mucho más confiable.
-   En UMT Mercator puedo filtrar con mayor facilidad los datos para eficientar mis cálculos.

Para hacer cálculo eficientes y confiables, puedo usar la siguiente lógica:

1.  Convertir mis datos a proyección UTM Mercator
2.  Filtrar datos de interés con criterio del cuadrado
3.  Regresar datos filtrados a coordenadas geográficas
4.  Calcular distancia Haversine

```{r}
# Objeto para almacenar datos
total3 <- c()

# Estampa de tiempo
c <- now()

# Procesamiento
# Primeros 5 delitos
for(i in 1:5){
  # Contador de delitos
  aux <- 0
  
  # Filtrar delitos en el cuadrado (Valor absoluto)
  vec1 <- which( abs(sp_geo$lat_UTM[i] - sp_geo$lat_UTM) < 100)
  vec2 <- which( abs(sp_geo$lon_UTM[i] - sp_geo$lon_UTM) < 100)
  vec <- intersect(vec1,vec2)
  
  # Filtrar delitos en el cuadrado pero fuera del círculo
  if(length(vec) > 0){
    for(j in 1:length(vec)){
      if(
        (distHaversine(
          c(B$longitud[vec[j]], B$latitud[vec[j]]),
          c(B$longitud[i],B$latitud[i]),
          r=6378137) < 100))
        {aux <- aux + 1}
    }
  }
  # Acumular delito en contador
  total3 <- rbind(total3,aux)
  print(i)
}

# Tiempo de procesamiento
c_ciclo <- now() - c
c_ciclo
```

Vemos que efectivamente hace el trabajo, y de igual forma lo realiza eficientemente. La diferencia en esta ocasión fue que se utilizó la distancia Haversine. Comparamos los resultados:

```{r}
resultados <- data.frame(
  for_hav=total,
  filtro_utm=total2,
  filtro_hav=total3)

resultados
```

Y vemos que los resultados entre el ciclo `for` con haversine coinciden con los resultados del filtro con haversine. Logramos obetener el mismo resultado, utilizando un código mucho más eficiente.

# Visualización de datos espaciales

## Herramientas web

Para la visualización y generación de mapas existen ya múltiples herramientas que nos pueden ayudar. Algunas son herramientas locales, pero también existen herramientas web, como podrían ser **myMaps** o **Earth** ambas de Google.

Estas dos en particular, requieren que la información esté en un tipo de archivo **.kml**. Toma los datos de las alcaldías por ejemplo, para generar este tipo de archivo utiliza los comandos:

```{r}
# Carga de datos
alcaldias_dir <- paste0(getwd(), '/datos/Alcaldias')
alcaldias <- paste0(getwd(), '/datos/Alcaldias/alcaldias.shp')

shape = readOGR(alcaldias, layer="alcaldias")

# Escribir KML
writeOGR(
  shape,
  dsn=paste0(alcaldias_dir, "/alc.kml"),
  layer="alc",
  driver="KML")
```

Estos datos los puedes cargar en las herramientas descritas:

-   [Google Earth](https://www.google.com.mx/earth/)
-   [myMaps](https://www.google.com/intl/es-419/maps/about/mymaps/)












